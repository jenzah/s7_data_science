{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# openpyxl-3.1.5\n",
    "# nbformat>=4.2.0\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "from A_data_cleaner_class import DataCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_excel(\n",
    "    r\"Z:\\2025_ml_marie\\TD0002697 - Liste des consommateurs électrique - ALLDIV no null.xlsx\",\n",
    "    keep_default_na=False,\n",
    "    na_values=['vides', '[E]', 'empty', '']\n",
    "    )\n",
    "print(f\"Loaded dataset: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Initialise the data cleaner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaner = DataCleaner(df)\n",
    "cleaner.get_summary()\n",
    "\n",
    "target = cleaner.target\n",
    "numerics = cleaner.numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_and_print_columns_compact(df):\n",
    "    \"\"\" Print column analysis in a compact format \"\"\"\n",
    "    for column in df.columns:\n",
    "        print(f\"\\n{column}\")\n",
    "        print(\"-\" * len(column))\n",
    "        print(f\"Type: {df[column].dtype}\")\n",
    "        print(f\"Nulls: {df[column].isnull().sum()}\")\n",
    "        print(f\"Unique: {df[column].nunique()}\")\n",
    "        print(\"Top values:\")\n",
    "        print(df[column].value_counts().head().to_string())\n",
    "\n",
    "# Example usage:\n",
    "analyse_and_print_columns_compact(cleaner.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Cleaning up column names**\n",
    "Remove unprintable characters from column names and strip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner.find_unprintable_columns()\n",
    "\n",
    "print(\"Cleaning column names...\")\n",
    "cleaner.standardise_column_names()\n",
    "print(\"✓ All column names are clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Drop columns that are not of interest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial column count: {len(cleaner.df.columns)}\")\n",
    "cleaner.drop_columns()\n",
    "print(f\"Final column count: {len(cleaner.df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Clean target - Allocated division**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Target column before cleaning:\")\n",
    "print(cleaner.df[target].value_counts(dropna=False))\n",
    "\n",
    "cleaner.clean_target(only_later=False)\n",
    "\n",
    "print(f\"\\nTarget column after cleaning:\")\n",
    "print(cleaner.df[target].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4: Trim whitespace**\n",
    "Remove leading/trailing whitespace from string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_df = cleaner.find_whitespace_in_values()\n",
    "\n",
    "if len(whitespace_df) > 0:\n",
    "    print(\"Columns with whitespace issues:\")\n",
    "    display(whitespace_df)\n",
    "    \n",
    "    # Trim whitespace from all string columns\n",
    "    cleaner.trim_whitespace()\n",
    "else:\n",
    "    print(\"✓ No whitespace issues found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 5: Standardise NA values**\n",
    "Transform NA variations into NA. Examples:\n",
    "- `N/A`\n",
    "- `not available`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_different_na = cleaner.NA_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 6: Standardise case**\n",
    "Fix case-insensitive duplicates (e.g., 'LATER' vs 'later')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_dups_df = cleaner.find_case_insensitive_duplicates()\n",
    "\n",
    "if len(case_dups_df) > 0:\n",
    "    print(\"Columns with case-insensitive duplicates:\")\n",
    "    display(case_dups_df)\n",
    "    \n",
    "    # Standardize case for affected columns\n",
    "    columns_to_standardise = case_dups_df['column'].tolist()\n",
    "    cleaner.standardise_case(columns_to_standardise)\n",
    "else:\n",
    "    print(\"✓ No case-insensitive duplicates found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 7: Find and fix fuzzy duplicates**\n",
    "Identify potential typos and similar strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_issues = cleaner.find_fuzzy_duplicates(threshold=85, min_length=3)\n",
    "\n",
    "if fuzzy_issues:\n",
    "    print(f\"Found fuzzy duplicates in {len(fuzzy_issues)} columns:\\n\")\n",
    "    for issue in fuzzy_issues:\n",
    "        print(f\"Column: {issue['column']}\")\n",
    "        for i, group in enumerate(issue['fuzzy_groups'], 1):\n",
    "            print(f\"  Group {i}: {group}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"✓ No fuzzy duplicates found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. comment utiliser\n",
    "# ELECTRICAL LOAD TYPE\n",
    "load_type_mapping = {\n",
    "    'ON OFF VALVE':'ON-OFF VALVE'\n",
    "}\n",
    "cleaner.standardise_fuzzy_values('ELECTRICAL LOAD TYPE', load_type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 8: Type conversion**\n",
    "Convert columns to appropriate data types (numeric, boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current data types:\")\n",
    "print(cleaner.get_summary())\n",
    "\n",
    "if numerics:\n",
    "    print(\"\\nConverting to numeric:\")\n",
    "    cleaner.convert_to_numeric()\n",
    "\n",
    "cleaner.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Review Data Quality After Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "dup_count = cleaner.df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {dup_count}\")  ## eventual duplicates come from dropping CL LINE\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nData type distribution:\")\n",
    "print(cleaner.df.dtypes.value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "missing = cleaner.df.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "print(f\"\\nColumns with missing values: {len(missing)}\")\n",
    "if len(missing) > 0:\n",
    "    print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 9: Handle Unexpected and Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9.4 Clean unexpected values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = cleaner.get_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find null-like values that aren't actually null (?, empty, etc.)\n",
    "null_like = cleaner.find_null_like_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9.5 Swap NaN with `unset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_missing = cleaner.get_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get non-numeric columns\n",
    "non_numeric_cols = cleaner.current_df.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Apply transformation\n",
    "cleaner.current_df[non_numeric_cols] = cleaner.current_df[non_numeric_cols].fillna(np.nan).replace(np.nan, \"LATER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner.current_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9.6 Clean textual columns**\n",
    "Trasform the following values into \"LATER\": '-', 'empty', 'à'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = cleaner.textual_columns(only_later=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Review data quality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "dup_count = cleaner.df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {dup_count}\")  ## eventual duplicates come from dropping CL LINE\n",
    "\n",
    "cleaner.get_summary()\n",
    "\n",
    "final_missing = cleaner.get_missing_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prepare dataset for mutual importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_distribution(df, column_name, title=None, color='#636EFA', \n",
    "                                  top_n=None, sort_by='count', save_path=None):\n",
    "    \"\"\"\n",
    "    Generate a bar chart showing the distribution of a categorical column using Plotly.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the data\n",
    "    column_name : str\n",
    "        The name of the categorical column to visualize\n",
    "    title : str, optional\n",
    "        Title for the chart (defaults to 'Distribution of {column_name}')\n",
    "    color : str, optional\n",
    "        Color for the bars\n",
    "    top_n : int, optional\n",
    "        Display only the top N categories (by count)\n",
    "    sort_by : str, optional\n",
    "        How to sort categories: 'count' (descending), 'alphabetical', or 'none'\n",
    "    save_path : str, optional\n",
    "        Path to save the figure (if None, figure is only displayed)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : plotly.graph_objects.Figure\n",
    "        The plotly figure object\n",
    "    \"\"\"\n",
    "    import plotly.express as px\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Set default title if not provided\n",
    "    if title is None:\n",
    "        title = f'Distribution of {column_name}'\n",
    "    \n",
    "    # Calculate value counts\n",
    "    value_counts = df[column_name].value_counts()\n",
    "    \n",
    "    # Apply top_n if specified\n",
    "    if top_n is not None and len(value_counts) > top_n:\n",
    "        top_values = value_counts.nlargest(top_n)\n",
    "        other_count = value_counts[top_n:].sum()\n",
    "        \n",
    "        # Add \"Other\" category\n",
    "        if other_count > 0:\n",
    "            data = pd.DataFrame({\n",
    "                'Category': list(top_values.index) + ['Other'],\n",
    "                'Count': list(top_values.values) + [other_count]\n",
    "            })\n",
    "        else:\n",
    "            data = pd.DataFrame({\n",
    "                'Category': top_values.index,\n",
    "                'Count': top_values.values\n",
    "            })\n",
    "    else:\n",
    "        data = pd.DataFrame({\n",
    "            'Category': value_counts.index,\n",
    "            'Count': value_counts.values\n",
    "        })\n",
    "    \n",
    "    # Sort the data\n",
    "    if sort_by == 'count':\n",
    "        data = data.sort_values('Count', ascending=False)\n",
    "    elif sort_by == 'alphabetical':\n",
    "        data = data.sort_values('Category')\n",
    "    # 'none' will use the order from value_counts\n",
    "    \n",
    "    # Create the figure\n",
    "    fig = px.bar(\n",
    "        data, x='Category', y='Count',\n",
    "        title=title,\n",
    "        color_discrete_sequence=[color]\n",
    "    )\n",
    "    \n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        xaxis_title=column_name,\n",
    "        yaxis_title='Count',\n",
    "        bargap=0.2,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    fig.update_traces(texttemplate='%{y}', textposition='outside')\n",
    "    \n",
    "    # Rotate x-axis labels if there are many categories\n",
    "    if len(data) > 5:\n",
    "        fig.update_layout(xaxis_tickangle=-45)\n",
    "    \n",
    "    # Save if path is provided\n",
    "    if save_path:\n",
    "        fig.write_image(save_path)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def analyse_mutual_information(df, target, top_n=20, show_plot=True):\n",
    "    \"\"\"\n",
    "    Analyse mutual information between features and target variable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        DataFrame containing the data\n",
    "    target : str\n",
    "        Name of the target column\n",
    "    top_n : int, default=20\n",
    "        Number of top features to return\n",
    "    show_plot : bool, default=True\n",
    "        Whether to display the plotly visualization\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_final, mi_df, top_features)\n",
    "        - X_final: DataFrame with top features and target\n",
    "        - mi_df: DataFrame with all MI scores and metadata\n",
    "        - top_features: List of top N feature names\n",
    "    \"\"\"\n",
    "    # After extraction, encode categorically\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Extract all meaningful parts\n",
    "    # df_clean['ECS_number'] = df_clean['ECS CODE'].str.extract(r'[A-Z]+(\\d{2})').fillna(\"unknown\")\n",
    "    # df_clean = df_clean.drop(columns=['ECS CODE', 'DESCRIPTION'])\n",
    "    df_clean = df_clean.drop(columns=['DESCRIPTION', 'ADDITIONAL REQUIREMENTS'], errors='ignore')\n",
    "    \n",
    "    y = df_clean[target]\n",
    "    X = df_clean.drop(columns=[target])\n",
    "    \n",
    "    # Prepare data and track which features are discrete/categorical\n",
    "    X_prepared = X.copy()\n",
    "    discrete_mask = []\n",
    "    \n",
    "    print(\"Preparing columns for mutual information...\")\n",
    "    for col in X_prepared.columns:\n",
    "        # Handle missing values\n",
    "        if X_prepared[col].dtype in ['object', 'string', 'category', 'boolean']:\n",
    "            # Categorical column - fill missing and convert to category codes\n",
    "            X_prepared[col] = X_prepared[col].fillna('_MISSING_').astype('category')\n",
    "            X_prepared[col] = X_prepared[col].cat.codes\n",
    "            discrete_mask.append(True)\n",
    "        elif X_prepared[col].dtype in ['int64', 'float64']:\n",
    "            # Numeric column\n",
    "            X_prepared[col] = X_prepared[col].fillna(-999)\n",
    "            # Consider integer columns as discrete if they have few unique values\n",
    "            if X_prepared[col].dtype == 'int64' or X_prepared[col].nunique() < 20:\n",
    "                discrete_mask.append(True)\n",
    "            else:\n",
    "                discrete_mask.append(False)\n",
    "        else:\n",
    "            # Other types - treat as categorical\n",
    "            X_prepared[col] = X_prepared[col].fillna('_MISSING_').astype('category').cat.codes\n",
    "            discrete_mask.append(True)\n",
    "    \n",
    "    print(f\"Discrete features: {sum(discrete_mask)}/{len(discrete_mask)}\")\n",
    "    \n",
    "    # Encode target if categorical\n",
    "    y_encoded = y.copy()\n",
    "    if y.dtype in ['object', 'string', 'category']:\n",
    "        y_encoded = y.astype('category').cat.codes\n",
    "        # Get category mapping\n",
    "        cat_mapping = dict(enumerate(y.astype('category').cat.categories))\n",
    "        print(f\"\\nTarget classes: {cat_mapping}\")\n",
    "    \n",
    "    # Calculate mutual information with discrete_features parameter\n",
    "    print(\"\\nCalculating mutual information...\")\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X_prepared, \n",
    "        y_encoded, \n",
    "        discrete_features=discrete_mask,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    mi_df = pd.DataFrame({\n",
    "        'feature': X_prepared.columns,\n",
    "        'mutual_information': mi_scores,\n",
    "        'dtype': [str(X[col].dtype) for col in X.columns],\n",
    "        'is_discrete': discrete_mask\n",
    "    }).sort_values('mutual_information', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MUTUAL INFORMATION SCORES\")\n",
    "    print(\"=\"*60)\n",
    "    print(mi_df.to_string(index=False))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Interactive Plotly visualization\n",
    "    if show_plot:\n",
    "        fig = px.bar(\n",
    "            mi_df,\n",
    "            x='mutual_information',\n",
    "            y='feature',\n",
    "            orientation='h',\n",
    "            title=f'Feature Importance: Mutual Information with {target}',\n",
    "            labels={'mutual_information': 'Mutual Information Score', 'feature': 'Feature'},\n",
    "            hover_data=['dtype', 'is_discrete'],\n",
    "            color='mutual_information',\n",
    "            color_continuous_scale='Viridis',\n",
    "            height=max(400, len(mi_df) * 25)\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            yaxis={'categoryorder': 'total ascending'},\n",
    "            showlegend=False,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    # Select top N features\n",
    "    top_features = mi_df.head(top_n)['feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nTop {top_n} features:\")\n",
    "    for i, feat in enumerate(top_features, 1):\n",
    "        row = mi_df[mi_df['feature'] == feat].iloc[0]\n",
    "        mi_score = row['mutual_information']\n",
    "        dtype = row['dtype']\n",
    "        is_discrete = row['is_discrete']\n",
    "        feature_type = 'discrete' if is_discrete else 'continuous'\n",
    "        print(f\"  {i}. {feat} ({dtype}, {feature_type}): {mi_score:.4f}\")\n",
    "    \n",
    "    # Create final dataset with top features\n",
    "    X_final = df_clean[top_features + [target]]\n",
    "    print(f\"\\nFinal dataset shape: {X_final.shape}\")\n",
    "    \n",
    "    return X_final, mi_df, top_features\n",
    "\n",
    "for col in cleaner.df.columns:\n",
    "    fig = plot_categorical_distribution(cleaner.df, col)\n",
    "    fig.show()\n",
    "    \n",
    "# One-line call with defaults\n",
    "X_final, mi_df, top_features = analyse_mutual_information(cleaner.df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def create_correlation_heatmap(df, title='Full Correlation Matrix (Numerical & Encoded Categorical Variables)',\n",
    "                               width=1000, height=900, show_plot=True):\n",
    "    \"\"\"\n",
    "    Create a correlation heatmap for a dataframe with both numerical and categorical columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe to analyse\n",
    "    title : str, optional\n",
    "        Title for the heatmap (default: 'Full Correlation Matrix...')\n",
    "    width : int, optional\n",
    "        Width of the figure in pixels (default: 1000)\n",
    "    height : int, optional\n",
    "        Height of the figure in pixels (default: 900)\n",
    "    show_plot : bool, optional\n",
    "        Whether to display the plot (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : plotly.graph_objects.Figure\n",
    "        The correlation heatmap figure\n",
    "    correlation_all : pandas.DataFrame\n",
    "        The full correlation matrix\n",
    "    \"\"\"    \n",
    "    # Create a copy of the dataframe\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = df_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_cols = df_encoded.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    print(f\"Encoding {len(categorical_cols)} categorical columns\")\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    for col in categorical_cols:\n",
    "        # Fill NaN values with a placeholder string to avoid errors\n",
    "        df_encoded[col] = df_encoded[col].fillna('missing')\n",
    "        df_encoded[col] = encoder.fit_transform(df_encoded[col].values.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_all = df_encoded.corr()\n",
    "    \n",
    "    # Create shortened labels for better visualization\n",
    "    def create_short_label(text):\n",
    "        words = str(text).split()\n",
    "        if len(words) <= 1:\n",
    "            return text\n",
    "        elif len(words) <= 3:  # Keep names with up to 3 words unchanged\n",
    "            return text\n",
    "        else:\n",
    "            return f\"{words[0]} {words[1]} {words[2]} ... {words[-1]}\"\n",
    "    \n",
    "    short_labels = {col: create_short_label(col) for col in correlation_all.columns}\n",
    "    correlation_short = correlation_all.rename(columns=short_labels, index=short_labels)\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = px.imshow(correlation_short, aspect='auto', title=title)\n",
    "    \n",
    "    # Create a proper 2D array of custom data for hover information\n",
    "    n = len(correlation_all.columns)\n",
    "    custom_data = np.empty((n, n), dtype='object')\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Add row and column full names to custom data\n",
    "            custom_data[i, j] = [correlation_all.index[i], correlation_all.columns[j]]\n",
    "    \n",
    "    # Add text and hover information\n",
    "    fig.update_traces(\n",
    "        text=correlation_all.round(2).values,\n",
    "        texttemplate='%{text}',\n",
    "        customdata=custom_data,\n",
    "        hovertemplate='Row: %{customdata[0]}<br>Column: %{customdata[1]}<br>Correlation: %{z:.2f}'\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        margin=dict(l=50, r=50, t=100, b=50)\n",
    "    )\n",
    "    \n",
    "    # Adjust text size and color for better readability\n",
    "    fig.update_traces(textfont=dict(size=10, color='black'))\n",
    "    \n",
    "    # Improve colorscale for better contrast\n",
    "    fig.update_traces(colorscale='RdBu_r', zmid=0)\n",
    "    \n",
    "    # Print dataframe composition\n",
    "    print(f\"\\nDataframe composition:\")\n",
    "    print(f\"- Total columns: {len(df.columns)}\")\n",
    "    print(f\"- Numerical columns: {len(numerical_cols)}\")\n",
    "    print(f\"- Categorical columns: {len(categorical_cols)}\")\n",
    "    \n",
    "    if show_plot:\n",
    "        fig.show()\n",
    "    \n",
    "    return fig, correlation_all\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "fig, corr_matrix = create_correlation_heatmap(cleaner.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only from CODE column\n",
    "ecs_extracted = cleaner.extract_codes(only_ecs=True)\n",
    "print(f\"Columns after extracting and dropping ECS CODE: {cleaner.df.columns}\")\n",
    "\n",
    "\n",
    "X_final, mi_df, top_features = analyse_mutual_information(cleaner.df, target)\n",
    "fig, corr_matrix = create_correlation_heatmap(cleaner.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes_extracted = cleaner.extract_codes(only_ecs=False)\n",
    "print(f\"Columns after extracting and dropping all codes: {cleaner.df.columns}\")\n",
    "\n",
    "X_final, mi_df, top_features = analyse_mutual_information(cleaner.df, target)\n",
    "fig, corr_matrix = create_correlation_heatmap(cleaner.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_categorical_distribution(cleaner.df, cleaner.target)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cleaner.df.columns:\n",
    "    fig = plot_categorical_distribution(cleaner.df, col)\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
