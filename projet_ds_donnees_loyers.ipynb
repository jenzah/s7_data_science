{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1c84f6",
   "metadata": {},
   "source": [
    "# Notebook 1 : Pr√©paration et nettoyage des donn√©es immobili√®res\n",
    "\n",
    "---\n",
    "\n",
    "## üìë Table des mati√®res\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Import des biblioth√®ques](#Import-des-biblioth√®ques)\n",
    "3. [Chargement des donn√©es brutes](#Chargement-des-donn√©es-brutes)\n",
    "4. [Exploration initiale des donn√©es](#Exploration-initiale-des-donn√©es)\n",
    "5. [Nettoyage des donn√©es](#Nettoyage-des-donn√©es)\n",
    "6. [Transformation et enrichissement](#Transformation-et-enrichissement)\n",
    "7. [Analyses statistiques descriptives](#Analyses-statistiques-descriptives)\n",
    "8. [Visualisations exploratoires](#Visualisations-exploratoires)\n",
    "9. [Export des donn√©es nettoy√©es](#Export-des-donn√©es-nettoy√©es)\n",
    "10. [Synth√®se du nettoyage](#Synth√®se-du-nettoyage)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Objectif de ce notebook\n",
    "\n",
    "Ce notebook a pour objectif de pr√©parer les donn√©es loyers.\n",
    "\n",
    "### Sources de donn√©es utilis√©es\n",
    "\n",
    "**\"Carte des loyers\" - Indicateurs de loyers d'annonce par commune en 2024, [source](https://www.data.gouv.fr/datasets/carte-des-loyers-indicateurs-de-loyers-dannonce-par-commune-en-2024/)**\n",
    "- d√©velopp√©e par la Direction G√©n√©rale de l'Am√©nagement, du Logement et de la Nature,\n",
    "- bas√©e sur 9,9 millions d'annonces locatives, permettant d'estimer les prix des loyers par commune pour le 3√®me trimestre 2024.\n",
    "\n",
    "Les indicateurs sont calcul√©s √† partir des donn√©es de leboncoin et SeLoger; ils couvrent toute la France et concernent diff√©rents types de logements.\n",
    "\n",
    "Le document comporte √©galement d'importantes mises en garde concernant l'utilisation de ces estimations, recommandant la prudence pour¬†:\n",
    "- Les communes avec peu d'observations (moins de 30)\n",
    "- Les zones avec un faible coefficient de d√©termination (R2 < 0,5)\n",
    "- Les zones avec des intervalles de pr√©diction tr√®s larges\n",
    "\n",
    "---\n",
    "\n",
    "## Import des biblioth√®ques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ecacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulation de donn√©es\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Tuple, List, Dict, List, Optional, Union\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Utilitaires\n",
    "from datetime import datetime\n",
    "from rapidfuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280d93d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Chargement des donn√©es brutes\n",
    "\n",
    "### Chargement et fusionnement des fichiers de loyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, name):\n",
    "    df = pd.read_csv(file_path,  encoding='latin1', sep=';')\n",
    "    # Add source column\n",
    "    df['Type de bien'] = name\n",
    "    return df\n",
    "\n",
    "def compare_columns(df1, df2, df3, df4):\n",
    "    print(\"Compare columns:\")\n",
    "    print(f\"Number of columns: {len(df1.columns)} == {len(df2.columns)} == {len(df3.columns)} == {len(df4.columns)}\")\n",
    "\n",
    "    for col in df1.columns:\n",
    "        in_df2 = col in df2.columns\n",
    "        in_df3 = col in df3.columns\n",
    "        in_df4 = col in df4.columns\n",
    "        if in_df2 and in_df3 and in_df4:\n",
    "            print(f\"   - {col} OK in all df\")\n",
    "        else:\n",
    "            print(f\"   - {col} MISSING in: \", end=\"\")\n",
    "            if not in_df2:\n",
    "                print(\"df2 \", end=\"\")\n",
    "            if not in_df3:\n",
    "                print(\"df3 \", end=\"\")\n",
    "            if not in_df4:\n",
    "                print(\"df4 \", end=\"\")\n",
    "    return\n",
    "\n",
    "def merge_datasets(df1, df2, df3, df4):\n",
    "    merged_df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "df_appartements = load_data(\"data/indicateurs_de_loyers_appartements.csv\", \"Appartement\")\n",
    "df_appartements3plus = load_data(\"data/indicateurs_de_loyers_appartements3plus.csv\", \"Appartement T3+\")\n",
    "df_appartements12 = load_data(\"data/indicateurs_de_loyers_appartements12.csv\", \"Appartement T1-T2\")\n",
    "df_maisons = load_data(\"data/indicateurs_de_loyers_maisons.csv\", \"Maison\")\n",
    "# compare_columns(df_appartements, df_appartements3plus, df_appartements12, df_maisons)\n",
    "\n",
    "merged_df = merge_datasets(df_appartements, df_appartements3plus, df_appartements12, df_maisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5333ce9",
   "metadata": {},
   "source": [
    "### Chargement des donn√©es compl√©mentaires (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Charger les donn√©es INSEE, API, etc. si applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c4e73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exploration initiale des donn√©es**\n",
    "\n",
    "### **Structure du dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1235b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(df, show_missing=False):\n",
    "    \"\"\"Get a summary of the current dataset state\"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    if show_missing:\n",
    "        print(f\"\\nMissing values:\")\n",
    "        missing = df.isnull().sum()\n",
    "        missing = missing[missing > 0].sort_values(ascending=False)\n",
    "        if len(missing) > 0:\n",
    "            for col, count in missing.items():\n",
    "                pct = (count / len(df)) * 100\n",
    "                print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
    "        else:\n",
    "            print(\"  No missing values!\")\n",
    "\n",
    "get_summary(merged_df, show_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80f294",
   "metadata": {},
   "source": [
    "#### **Analyse des valeurs manquantes**\n",
    "Il n'y a aucune valeur manquante dans ces ensembles de donn√©es.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analyse des colonnes - Colonnes de la Carte des Loyers 2024**\n",
    "\n",
    "- `id_zone` : Identifiant unique de la zone\n",
    "- `INSEE_C` : Code INSEE de la commune\n",
    "- `LIBGEO` : Nom de la zone g√©ographique (commune)\n",
    "- `EPCI` : Code de groupement intercommunal\n",
    "- `DEP` : Code du d√©partement\n",
    "- `REG` : Code de la r√©gion\n",
    "- `loypredm2` : Prix de location pr√©dit par m¬≤\n",
    "- `lwr.IPm2` : Borne inf√©rieure de l'intervalle de pr√©diction\n",
    "- `upr.IPm2` : Borne sup√©rieure de l'intervalle de pr√©diction\n",
    "- `TYPPRED` : Type de pr√©diction (\"commune\" ou \"maille\")\n",
    "- `nbobs_com` : Nombre d'observations dans la commune\n",
    "- `nbobs_mail` : Nombre d'observations dans la zone plus large\n",
    "- `R2_adj` : R¬≤ ajust√© (mesure statistique de l'ajustement)\n",
    "\n",
    "#### **Pr√©cautions d'utilisation**\n",
    "√ätre prudent avec les donn√©es o√π :\n",
    "- R2_adj < 0,5\n",
    "- nbobs_com < 30\n",
    "- L'intervalle de pr√©diction est tr√®s large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c273eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_and_print_columns_compact(df):\n",
    "    \"\"\" Print column analysis in a compact format \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"COLUMNS ANALYSIS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"\\nList of columns: {', '.join(df.columns)}\")\n",
    "    print(\"\\n\")\n",
    "    for column in df.columns:\n",
    "        print(\"-\" * 2*len(column))\n",
    "        print(f\"{column}\")\n",
    "        print(\"-\" * 2*len(column))\n",
    "        print(f\"Type: {df[column].dtype}\")\n",
    "        print(f\"Nulls: {df[column].isnull().sum()}\")\n",
    "        print(f\"Unique: {df[column].nunique()}\")\n",
    "        print(\"Top values:\")\n",
    "        print(df[column].value_counts().head().to_string())\n",
    "        print(\"\\n\")\n",
    "\n",
    "analyse_and_print_columns_compact(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d043b",
   "metadata": {},
   "source": [
    "#### **Analyse des colonnes**\n",
    "D'apr√®s l'analyse, nous constatons que les colonnes suivantes n√©cessiteront un changement de type :\n",
    "- `loypredm2` --> num√©rique\n",
    "- `lwr.IPm2` --> num√©rique\n",
    "- `upr.IPm2` --> num√©rique\n",
    "- `R2_adj` --> num√©rique\n",
    "- `REG` --> cat√©goriel\n",
    "\n",
    "---\n",
    "\n",
    "## **Nettoyage des donn√©es**\n",
    "\n",
    "### **Standardisation des noms de colonnes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col_name(col: str) -> str:\n",
    "    \"\"\"Internal helper to clean a single column name.\"\"\"\n",
    "    cleaned_col = \"\".join(char if char.isprintable() else ' ' for char in col)\n",
    "    cleaned_col = re.sub(r'\\s+', ' ', cleaned_col)\n",
    "    return cleaned_col.strip()\n",
    "\n",
    "def find_unprintable_columns(df):\n",
    "    \"\"\"Find columns with unprintable characters or whitespace issues in names\"\"\"\n",
    "    issues = []\n",
    "    for col in df.columns:\n",
    "        problems = []\n",
    "        # Check for unprintable characters\n",
    "        if not col.isprintable():\n",
    "            problems.append(\"unprintable characters\")\n",
    "        # Check for leading/trailing whitespace\n",
    "        if col != col.strip():\n",
    "            problems.append(\"leading/trailing whitespace\")\n",
    "        # Check for internal multiple spaces\n",
    "        if '  ' in col:\n",
    "            problems.append(\"multiple internal spaces\")\n",
    "        # Check for tabs\n",
    "        if '\\t' in col:\n",
    "            problems.append(\"tab characters\")\n",
    "        # Check for newlines\n",
    "        if '\\n' in col or '\\r' in col:\n",
    "            problems.append(\"newline characters\")\n",
    "        if problems:\n",
    "            issues.append({\n",
    "                'column': repr(col),\n",
    "                'issues': ', '.join(problems),\n",
    "                'cleaned_version': clean_col_name(col)\n",
    "            })\n",
    "    if issues:\n",
    "        print(f\"Found {len(issues)} columns with issues:\")\n",
    "        for item in issues:\n",
    "            print(f\"  Column: {item['column']}\")\n",
    "            print(f\"    Issues: {item['issues']}\")\n",
    "            print(f\"    Will become: '{item['cleaned_version']}'\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"‚úì All column names are clean!\")\n",
    "    return issues\n",
    "\n",
    "def standardise_column_names(df):\n",
    "    \"\"\"Standardise column names by removing control characters like \\\\n, \\\\t\"\"\"\n",
    "    old_cols = df.columns.tolist()\n",
    "    df.columns = [clean_col_name(col) for col in df.columns]\n",
    "    changed = sum(1 for old, new in zip(old_cols, df.columns) if old != new)\n",
    "    print(f\"‚úì Cleaned {changed} column names\")\n",
    "    return df\n",
    "    \n",
    "issues = find_unprintable_columns(merged_df)\n",
    "\n",
    "if issues:\n",
    "    merged_df = standardise_column_names(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a2bfa",
   "metadata": {},
   "source": [
    "### **S√©lection des colonnes pertinentes**\n",
    "\n",
    "On commence en filtrant sur la r√©gion pertinante.\n",
    "\n",
    "**Colonnes essentielles**\n",
    "- `LIBGEO` : Nom de la commune (pour visualisation)\n",
    "- `INSEE_C` : Code INSEE (pour jointures)\n",
    "- `DEP` : Code d√©partement\n",
    "- `loypredm2` : Prix de location pr√©dit par m¬≤\n",
    "\n",
    "**Colonnes pour l'analyse de qualit√©**\n",
    "- `nbobs_com` : Nombre d'observations dans la commune\n",
    "- `R2_adj` : Qualit√© statistique de la pr√©diction\n",
    "- `TYPPRED` : Type de pr√©diction (\"commune\" ou \"maille\")\n",
    "\n",
    "**Colonnes d'intervalles**\n",
    "- `lwr.IPm2` : Borne inf√©rieure de l'estimation\n",
    "- `upr.IPm2` : Borne sup√©rieure de l'estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"id_zone\", \"EPCI\"]\n",
    "\n",
    "def drop_columns(df):\n",
    "    \"\"\"Drop columns that are of no interest\"\"\"\n",
    "    dropped = []\n",
    "    for col in df.columns:\n",
    "        if col in columns_to_drop:\n",
    "            df = df.drop(columns=[col])\n",
    "            dropped.append(col)\n",
    "    if len(dropped) > 0:\n",
    "        print(f\"‚úì Dropped columns: {', '.join(dropped)}\")\n",
    "    else:\n",
    "        print(\"‚úì No columns to drop\")\n",
    "    return df\n",
    "\n",
    "merged_df = drop_columns(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00efc8a",
   "metadata": {},
   "source": [
    "### **Conversion des types de donn√©es**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\"loypredm2\", \"lwr.IPm2\", \"upr.IPm2\", \"R2_adj\"]\n",
    "categorial_columns = [\"REG\"]\n",
    "\n",
    "def convert_to_numeric(df):\n",
    "    \"\"\"Converts specified columns to numeric type\"\"\"\n",
    "    converted = []\n",
    "    for col in numeric_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"‚ö† Column '{col}' not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col].astype(str).str.replace(',', '.', regex=False), \n",
    "                errors='coerce'\n",
    "            )\n",
    "            converted.append(col)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Error converting '{col}': {e}\")\n",
    "    \n",
    "    print(f\"‚úì Converted {len(converted)} columns to numeric type\")\n",
    "    return df\n",
    "\n",
    "def convert_to_categorial(df):\n",
    "    \"\"\"Converts specified columns to categorial type\"\"\"\n",
    "    converted = []\n",
    "    for col in categorial_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"‚ö† Column '{col}' not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            df[col] = df[col].astype('object')\n",
    "            converted.append(col)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Error converting '{col}': {e}\")\n",
    "    \n",
    "    print(f\"‚úì Converted {len(converted)} columns to categorial type\")\n",
    "    return df\n",
    "\n",
    "merged_df = convert_to_numeric(merged_df)\n",
    "merged_df = convert_to_categorial(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e96fe4",
   "metadata": {},
   "source": [
    "### **Suppression des espaces superflus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a81812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_whitespace_in_values(df):\n",
    "    \"\"\"Find columns with leading/trailing whitespace in values\"\"\"\n",
    "    whitespace_info = []\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    for col in string_cols:\n",
    "        has_whitespace = df[col].astype(str).str.strip() != df[col].astype(str)\n",
    "        if has_whitespace.any():\n",
    "            count = has_whitespace.sum()\n",
    "            whitespace_mask = has_whitespace\n",
    "            examples_original = df[col][whitespace_mask].head(3).tolist()\n",
    "            examples_cleaned = [str(val).strip() for val in examples_original]\n",
    "            examples_orig_str = ' | '.join([f'\"{val}\"' for val in examples_original])\n",
    "            examples_clean_str = ' | '.join([f'\"{val}\"' for val in examples_cleaned])\n",
    "            whitespace_info.append({\n",
    "                'column': col,\n",
    "                'affected_rows': count,\n",
    "                'percentage': round(count / len(df) * 100, 2),\n",
    "                'examples_before': examples_orig_str,\n",
    "                'examples_after': examples_clean_str\n",
    "            })\n",
    "    return pd.DataFrame(whitespace_info)\n",
    "\n",
    "def trim_whitespace(df, columns=None):\n",
    "    \"\"\"Trim whitespace from specified columns (or all string columns if None)\"\"\"\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    if columns is not None:\n",
    "        string_cols = [col for col in columns if col in string_cols]\n",
    "    \n",
    "    for col in string_cols:\n",
    "        df[col] = df[col].str.strip()\n",
    "    \n",
    "    print(f\"‚úì Trimmed whitespace from {len(string_cols)} columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "whitespace_df = find_whitespace_in_values(merged_df)\n",
    "\n",
    "if len(whitespace_df) > 0:\n",
    "    print(\"Columns with whitespace issues:\")\n",
    "    display(whitespace_df)\n",
    "    \n",
    "    # Trim whitespace from all string columns\n",
    "    trim_whitespace(merged_df)\n",
    "else:\n",
    "    print(\"‚úì No whitespace issues found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4f632",
   "metadata": {},
   "source": [
    "### **Harmonisation des majuscules/minuscules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81749e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_case_insensitive_duplicates(df):\n",
    "    \"\"\"\n",
    "    Finds columns with case-insensitive duplicates (e.g., 'Apple', 'apple').\n",
    "    Returns a DataFrame summarizing the issues for easy display.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    for col in string_cols:\n",
    "        series = df[col]\n",
    "        clean_series = series.dropna().astype(str)\n",
    "        if len(clean_series) == 0:\n",
    "            continue\n",
    "        case_map = {}\n",
    "        for value in clean_series.unique():\n",
    "            lower_val = value.lower()\n",
    "            if lower_val in case_map:\n",
    "                case_map[lower_val].append(value)\n",
    "            else:\n",
    "                case_map[lower_val] = [value]\n",
    "        duplicate_groups = [group for group in case_map.values() if len(group) > 1]\n",
    "        if duplicate_groups:\n",
    "            value_counts = df[col].value_counts()\n",
    "            total_affected_rows = 0\n",
    "            example_groups = []\n",
    "            for group in duplicate_groups:\n",
    "                total_affected_rows += value_counts[group].sum()\n",
    "                most_frequent_form = max(group, key=lambda x: value_counts.get(x, 0))\n",
    "                group_str = ' | '.join([f'\"{val}\"' for val in sorted(group)])\n",
    "                example_groups.append(f'{group_str} -> \"{most_frequent_form}\"')\n",
    "            results.append({\n",
    "                'column': col,\n",
    "                'duplicate_groups': len(duplicate_groups),\n",
    "                'affected_rows': total_affected_rows,\n",
    "                'examples': ' || '.join(example_groups[:3])\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def standardise_case(df, columns: list):\n",
    "    \"\"\"\n",
    "    Standardises the casing of values in the specified columns.\n",
    "    \"\"\"\n",
    "    standardised_count = 0\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        series = df[col]\n",
    "        clean_series = series.dropna().astype(str)\n",
    "        if len(clean_series) == 0:\n",
    "            continue\n",
    "        case_map = {}\n",
    "        for value in clean_series.unique():\n",
    "            lower_val = value.lower()\n",
    "            if lower_val in case_map:\n",
    "                case_map[lower_val].append(value)\n",
    "            else:\n",
    "                case_map[lower_val] = [value]\n",
    "        duplicate_groups = [group for group in case_map.values() if len(group) > 1]\n",
    "        if not duplicate_groups:\n",
    "            continue\n",
    "        value_counts = df[col].value_counts()\n",
    "        replacement_map = {}\n",
    "        for group in duplicate_groups:\n",
    "            most_frequent_form = max(group, key=lambda x: value_counts.get(x, 0))\n",
    "            for variant in group:\n",
    "                if variant != most_frequent_form:\n",
    "                    replacement_map[variant] = most_frequent_form\n",
    "        if replacement_map:\n",
    "            df[col] = df[col].replace(replacement_map)\n",
    "            standardised_count += 1\n",
    "    \n",
    "    print(f\"‚úì Standardised case in {standardised_count} columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "case_dups_df = find_case_insensitive_duplicates(merged_df)\n",
    "\n",
    "if len(case_dups_df) > 0:\n",
    "    print(\"Columns with case-insensitive duplicates:\")\n",
    "    display(case_dups_df)\n",
    "    \n",
    "    # Standardize case for affected columns\n",
    "    columns_to_standardise = case_dups_df['column'].tolist()\n",
    "    standardise_case(columns_to_standardise)\n",
    "else:\n",
    "    print(\"‚úì No case-insensitive duplicates found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557cdb4d",
   "metadata": {},
   "source": [
    "### **Trouver des groupes de cha√Ænes similaires (potentielles erreurs de frappe) dans les colonnes cat√©gorielles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ac260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_for_comparison(s: str) -> str:\n",
    "    \"\"\"Intelligently cleans a string for a base similarity comparison.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s_lower = s.lower()\n",
    "    s_lower = re.sub(r'tbc\\s*\\(proposition\\s*-?|local\\s*|√†\\s*confirmer|pp\\s*\\d', '', s_lower)\n",
    "    s_lower = re.sub(r'[\\s-]+', '', s_lower)\n",
    "    s_lower = s_lower.strip(\"()[]{}'\\\"- \")\n",
    "    return s_lower\n",
    "\n",
    "def find_fuzzy_duplicates(df, threshold: int = 85, min_length: int = 3):\n",
    "    \"\"\"\n",
    "    Finds groups of similar strings (potential typos) in categorical columns.\n",
    "    \"\"\"\n",
    "    issue_list = []\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    for col in string_cols:\n",
    "        series = df[col]\n",
    "        if series.nunique() < 2 or series.nunique() > 2000:\n",
    "            continue\n",
    "        categories = series.dropna().unique().tolist()\n",
    "        filtered_cats = [\n",
    "            cat for cat in set(categories)\n",
    "            if isinstance(cat, str) and len(cat) >= min_length and not re.search(r'\\d', cat)\n",
    "        ]\n",
    "        if len(filtered_cats) < 2:\n",
    "            continue\n",
    "        normalised_cats = [normalise_for_comparison(cat) for cat in filtered_cats]\n",
    "        score_matrix = process.cdist(normalised_cats, normalised_cats, scorer=fuzz.ratio, score_cutoff=threshold)\n",
    "        groups = []\n",
    "        processed_indices = set()\n",
    "        for i in range(len(filtered_cats)):\n",
    "            if i in processed_indices:\n",
    "                continue\n",
    "            nonzero_result = score_matrix[i].nonzero()\n",
    "            if isinstance(nonzero_result, tuple) and len(nonzero_result) > 0:\n",
    "                similar_indices = nonzero_result[0] if len(nonzero_result) == 1 else nonzero_result[1]\n",
    "            else:\n",
    "                continue\n",
    "            if len(similar_indices) > 1:\n",
    "                current_group = {filtered_cats[j] for j in similar_indices}\n",
    "                groups.append(sorted(list(current_group)))\n",
    "                processed_indices.update(similar_indices)\n",
    "        if groups:\n",
    "            issue_list.append({'column': col, 'fuzzy_groups': groups})\n",
    "    return issue_list\n",
    "\n",
    "fuzzy_issues = find_fuzzy_duplicates(merged_df, threshold=85, min_length=3)\n",
    "\n",
    "if fuzzy_issues:\n",
    "    print(f\"Found fuzzy duplicates in {len(fuzzy_issues)} columns:\\n\")\n",
    "    for issue in fuzzy_issues:\n",
    "        print(f\"Column: {issue['column']}\")\n",
    "        for i, group in enumerate(issue['fuzzy_groups'], 1):\n",
    "            print(f\"  Group {i}: {group}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚úì No fuzzy duplicates found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e29d39",
   "metadata": {},
   "source": [
    "### **Suppression des doublons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057efa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    \"\"\"Removes duplicate rows from the current DataFrame.\"\"\"\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates(ignore_index=True)\n",
    "    removed_rows = initial_rows - len(df)\n",
    "    print(f\"‚úì Removed {removed_rows} duplicate rows (kept {len(df)} unique rows)\")\n",
    "    return df\n",
    "\n",
    "merged_df = remove_duplicates(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d111a4",
   "metadata": {},
   "source": [
    "### **Filtrage des donn√©es**\n",
    "Selon deux crit√®res principaux :\n",
    "\n",
    "**Filtrage g√©ographique pour l'√éle-de-France**\n",
    "- S√©lection des communes de la r√©gion √éle-de-France\n",
    "- Codes r√©gion : REG = \"11\"\n",
    "- Codes d√©partements : 75 (Paris), 77, 78, 91, 92, 93, 94, 95\n",
    "\n",
    "\n",
    "**Filtrage qualitatif des donn√©es**\n",
    "- Nombre minimal d'observations : nbobs_com ‚â• 30\n",
    "- Qualit√© statistique : R2_adj ‚â• 0,5\n",
    "- Type de pr√©diction : TYPPRED = \"commune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e46b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_region(df):\n",
    "    \"\"\"Filter for Ile-de-France region.\"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"FILTER FOR ILE-DE-FRANCE\")\n",
    "    print(\"=\"*100)\n",
    "    filtered_df = df[df[\"REG\"] == \"11\"]\n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(f\"IDF dataset size: {len(filtered_df)}\")\n",
    "    departments = sorted(filtered_df[\"DEP\"].unique().tolist())\n",
    "    print(f\"\\nDepartements in IDF: {', '.join(departments)}\")\n",
    "    types = filtered_df[\"Type de bien\"].unique().tolist()\n",
    "    print(f\"\\nTypes of property: {', '.join(types)}\")\n",
    "    return filtered_df\n",
    "\n",
    "idf_data = merged_df.copy()\n",
    "idf_data = filter_region(idf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b96561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers(df):\n",
    "    \"\"\"Filter out outliers defined by the dataset owners.\"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"FILTER OUT OUTLIERS\")\n",
    "    print(\"=\"*100)\n",
    "    mask = (\n",
    "        (df['nbobs_com'] >= 15) &\n",
    "        (df['R2_adj'] >= 0.5) &\n",
    "        (df['TYPPRED'] == 'commune')\n",
    "    )\n",
    "    filtered_df = df[mask]\n",
    "    \n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(f\"IDF dataset size: {len(filtered_df)}\")\n",
    "    departments = sorted(filtered_df[\"DEP\"].unique().tolist())\n",
    "    if departments:\n",
    "        print(f\"\\nDepartements in IDF: {', '.join(departments)}\")\n",
    "    types = filtered_df[\"Type de bien\"].unique().tolist()\n",
    "    if types:\n",
    "        print(f\"\\nTypes of property: {', '.join(types)}\")\n",
    "\n",
    "def analyze_outliers(df):\n",
    "    \"\"\"Analyze outliers defined by the dataset owners.\"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"ANALYZE OUTLIERS\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Low observation count\n",
    "    low_obs = df[df['nbobs_com'] < 10]\n",
    "    print(f\"Communes with less than 10 observations: {len(low_obs)} ({len(low_obs)/len(df)*100:.2f}%)\")\n",
    "    print(\"Sample of low observation communes:\")\n",
    "    print(low_obs[['LIBGEO', 'DEP', 'nbobs_com']].head())\n",
    "    \n",
    "    # Low R2 adjustment\n",
    "    low_r2 = df[df['R2_adj'] < 0.5]\n",
    "    print(f\"\\nCommunues with R2_adj < 0.5: {len(low_r2)} ({len(low_r2)/len(df)*100:.2f}%)\")\n",
    "    print(\"Sample of low R2 communes:\")\n",
    "    print(low_r2[['LIBGEO', 'DEP', 'R2_adj']].head())\n",
    "    \n",
    "    # Non-commune predictions\n",
    "    non_commune = df[df['TYPPRED'] != 'commune']\n",
    "    print(f\"\\nNon-commune predictions: {len(non_commune)} ({len(non_commune)/len(df)*100:.2f}%)\")\n",
    "    print(\"Sample of non-commune predictions:\")\n",
    "    print(non_commune[['LIBGEO', 'DEP', 'TYPPRED']].head())\n",
    "    \n",
    "    # Create a summary mask\n",
    "    mask = (\n",
    "        (df['nbobs_com'] >= 30) &\n",
    "        (df['R2_adj'] >= 0.5) &\n",
    "        (df['TYPPRED'] == 'commune')\n",
    "    )\n",
    "    filtered_df = df[mask]\n",
    "    \n",
    "    print(f\"\\nOriginal dataset size: {len(df)}\")\n",
    "    print(f\"Filtered dataset size: {len(filtered_df)} ({len(filtered_df)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    departments = sorted(filtered_df[\"DEP\"].unique().tolist())\n",
    "    if departments:\n",
    "        print(f\"\\nDepartements in filtered dataset: {', '.join(departments)}\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "analyze_outliers(idf_data)\n",
    "# idf_data = filter_outliers(idf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa41d5c",
   "metadata": {},
   "source": [
    "### Filtrage par type de bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Garder uniquement les types de biens pertinents\n",
    "# Exemple: appartements et maisons uniquement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7858b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Transformation et enrichissement\n",
    "\n",
    "### Cr√©ation de variables d√©riv√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Cr√©er des variables calcul√©es utiles\n",
    "\n",
    "# Prix au m¬≤\n",
    "\n",
    "# Ann√©e de transaction\n",
    "\n",
    "# Trimestre\n",
    "\n",
    "# R√©gion (√† partir du code d√©partement)\n",
    "# CODEZ ICI: Mapper les d√©partements aux r√©gions\n",
    "\n",
    "# Cat√©gorie de surface\n",
    "# CODEZ ICI: Cr√©er des cat√©gories (petit, moyen, grand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b5005",
   "metadata": {},
   "source": [
    "### Encodage des variables cat√©gorielles (si n√©cessaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a272e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Encoder les variables cat√©gorielles si n√©cessaire pour l'analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4cfbc",
   "metadata": {},
   "source": [
    "### Ajout de donn√©es externes (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Fusionner avec donn√©es INSEE, API transport, etc.\n",
    "# Exemple: ajouter population, revenu m√©dian par commune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263088a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analyses statistiques descriptives\n",
    "\n",
    "### Statistiques globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Statistiques descriptives g√©n√©rales\n",
    "\n",
    "# CODEZ ICI: Statistiques par cat√©gorie (type de bien, r√©gion, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f14a43",
   "metadata": {},
   "source": [
    "### Analyses par dimensions\n",
    "\n",
    "#### Par type de bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe217ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Statistiques par type de bien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af320d",
   "metadata": {},
   "source": [
    "#### Par ann√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: √âvolution temporelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9831fe",
   "metadata": {},
   "source": [
    "#### Par r√©gion/d√©partement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae075102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Statistiques g√©ographiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994df5da",
   "metadata": {},
   "source": [
    "### Distribution des variables cl√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6eda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Analyser la distribution des variables num√©riques importantes\n",
    "# - Distribution des prix\n",
    "# - Distribution des surfaces\n",
    "# - Distribution du prix au m¬≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc58f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualisations exploratoires\n",
    "\n",
    "### Distribution des prix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1200e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Histogramme de la distribution des prix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e133a",
   "metadata": {},
   "source": [
    "### Distribution des surfaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Histogramme de la distribution des surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453374f6",
   "metadata": {},
   "source": [
    "### Prix au m¬≤ par type de bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Boxplot comparant les prix au m¬≤ par type de bien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0392ebb1",
   "metadata": {},
   "source": [
    "### √âvolution temporelle des prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbfd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Graphique lin√©aire de l'√©volution des prix moyens par ann√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53740a5",
   "metadata": {},
   "source": [
    "### R√©partition g√©ographique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Top 10 ou Top 20 des d√©partements/villes par nombre de transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88d3d9",
   "metadata": {},
   "source": [
    "### Prix moyen par d√©partement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Carte ou graphique en barres des prix moyens par d√©partement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51431195",
   "metadata": {},
   "source": [
    "### Corr√©lations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Matrice de corr√©lation des variables num√©riques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60624912",
   "metadata": {},
   "source": [
    "### Prix au m¬≤ par r√©gion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Graphique comparant les prix au m¬≤ entre r√©gions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e715f69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export des donn√©es nettoy√©es\n",
    "\n",
    "### Sauvegarde du dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: Exporter le dataframe nettoy√©\n",
    "# df.to_csv('donnees_nettoyees.csv', index=False)\n",
    "# print(f\"Dataset nettoy√© export√© : {df.shape[0]} lignes, {df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d437c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Synth√®se du nettoyage\n",
    "\n",
    "### R√©sum√© des transformations effectu√©es\n",
    "\n",
    "<!-- COMPL√âTEZ ICI: R√©sumez toutes les √©tapes de nettoyage -->\n",
    "<!-- 1. Donn√©es brutes initiales : X lignes -->\n",
    "<!-- 2. Apr√®s suppression des valeurs manquantes : Y lignes -->\n",
    "<!-- 3. Apr√®s filtrage des aberrations : Z lignes -->\n",
    "<!-- 4. Variables cr√©√©es : liste -->\n",
    "<!-- 5. Donn√©es finales : N lignes, M colonnes -->\n",
    "\n",
    "### Qualit√© des donn√©es finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26116e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODEZ ICI: V√©rification finale de la qualit√©\n",
    "# - Pas de valeurs manquantes sur colonnes critiques\n",
    "# - Types de donn√©es corrects\n",
    "# - Plages de valeurs coh√©rentes\n",
    "\n",
    "# print(\"V√©rification finale :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a182e",
   "metadata": {},
   "source": [
    "### Recommandations pour l'analyse\n",
    "\n",
    "<!-- COMPL√âTEZ ICI: Notez les points importants pour l'analyse suivante -->\n",
    "<!-- - Variables les plus pertinentes identifi√©es -->\n",
    "<!-- - Limitations des donn√©es -->\n",
    "<!-- - Suggestions pour les widgets -->\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook pr√©par√© par :**\n",
    "- Ashley OHNONA\n",
    "- Harisoa RANDRIANASOLO\n",
    "- Fairouz YOUDARENE\n",
    "- Jennifer ZAHORA\n",
    "\n",
    "**Date :** <!-- COMPL√âTEZ ICI: Date -->\n",
    "\n",
    "**Dataset final :** `donnees_nettoyees.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
